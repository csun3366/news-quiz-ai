import os
import sys
import django
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# 1. è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„ (å‡è¨­ crawler.py åœ¨ core/ï¼Œsettings.py åœ¨ backend/)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# 2. è¨­å®š Django ç’°å¢ƒè®Šæ•¸
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'backend.settings')

# 3. å•Ÿå‹• Django
django.setup()

# 4. åŒ¯å…¥ä½ çš„ Model
from core.models import Article


def fetch_cnn_articles(category='business', limit=10):
    base_url = "https://edition.cnn.com"
    category_url = f"{base_url}/{category}"
    headers = {"User-Agent": "Mozilla/5.0"}

    print(f"ğŸŒ æ­£åœ¨çˆ¬å–åˆ†é¡ï¼š{category} ...")
    response = requests.get(category_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # CNN æ¨™é¡Œé¸æ“‡å™¨ (æœ€æ–°)
    headline_tags = soup.select('.container_lead-plus-headlines__headline span')

    added_count = 0
    for headline in headline_tags:
        if added_count >= limit:
            break

        title = headline.get_text(strip=True)
        if not title:
            continue

        # æ‰¾åˆ°è©²æ¨™é¡Œå°æ‡‰çš„ <a> é€£çµ
        link_tag = headline.find_parent('a')
        if not link_tag:
            continue

        href = link_tag.get('href')
        if not href.startswith('/'):
            continue

        url = base_url + href

        # é¿å…é‡è¤‡
        if Article.objects.filter(title=title).exists() or Article.objects.filter(url=url).exists():
            print(f"âš ï¸ æ–‡ç« å·²å­˜åœ¨ï¼Œè·³éï¼š{title}")
            continue

        # æŠ“å–æ–‡ç« å…§å®¹
        article_res = requests.get(url, headers=headers)
        article_soup = BeautifulSoup(article_res.text, 'html.parser')
        paragraphs = article_soup.select("article p") or article_soup.select("div.l-container p")
        content = '\n'.join(p.get_text(strip=True) for p in paragraphs)

        if not content.strip():
            print(f"âš ï¸ æ–‡ç« å…§å®¹ç‚ºç©ºï¼Œè·³éï¼š{title}")
            continue

        summary = content[:150] + '...' if len(content) > 150 else content

        # å„²å­˜åˆ°è³‡æ–™åº«
        Article.objects.create(
            title=title,
            summary=summary,
            content=content,
            url=url,
            source='CNN',
            category=category,
            published_at=datetime.now()
        )
        added_count += 1
        print(f"âœ… æ–°å¢æ–‡ç« ï¼š{title}")

    print(f"ğŸ¯ {category} é¡åˆ¥æ–°å¢å®Œæˆï¼Œå…±æ–°å¢ {added_count} ç¯‡æ–‡ç« ã€‚")


# ä½¿ç”¨æ–¹å¼: python cnn_crawler.py
if __name__ == '__main__':
    categories = ['world', 'business', 'tech', 'entertainment', 'sport']
    for cat in categories:
        fetch_cnn_articles(category=cat, limit=10)
